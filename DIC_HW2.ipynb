{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part 1.1"
      ],
      "metadata": {
        "id": "vN9zuLtZRPBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGHKO0F0_iT9",
        "outputId": "d5ce2452-79eb-462b-af9a-1fdde42dbc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 4811|\n",
            "|  to| 4383|\n",
            "|  of| 3955|\n",
            "| and| 3740|\n",
            "| her| 2258|\n",
            "|   a| 2079|\n",
            "|  in| 2038|\n",
            "|  94| 1899|\n",
            "| was| 1870|\n",
            "|   i| 1794|\n",
            "| she| 1710|\n",
            "|that| 1561|\n",
            "|  it| 1542|\n",
            "| not| 1508|\n",
            "| you| 1323|\n",
            "|  he| 1318|\n",
            "|  be| 1279|\n",
            "| his| 1279|\n",
            "|  as| 1223|\n",
            "| had| 1179|\n",
            "+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, lower, col, count\n",
        "from operator import add\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"GutenbergWordCount\").getOrCreate()\n",
        "\n",
        "# Read the text file\n",
        "file_path = \"PRIDE AND PREJUDICE.rtf\"\n",
        "text_file = spark.read.text(file_path)\n",
        "\n",
        "# Perform word count\n",
        "word_counts = (text_file\n",
        "    .select(explode(split(lower(col(\"value\")), \"\\\\W+\")).alias(\"word\"))\n",
        "    .filter(col(\"word\") != \"\")\n",
        "    .groupBy(\"word\")\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "    .orderBy(\"count\", ascending=False)\n",
        ")\n",
        "\n",
        "# Show the top 20 words\n",
        "word_counts.show(20)\n",
        "\n",
        "# Stop the Spark session\n",
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1.2"
      ],
      "metadata": {
        "id": "A8rWlfp_RVIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, lower, regexp_replace, col, count, size, trim\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"GutenbergWordCountExtended\").getOrCreate()\n",
        "\n",
        "# Read the text file\n",
        "file_path = \"PRIDE AND PREJUDICE.rtf\"\n",
        "text_file = spark.read.text(file_path)\n",
        "\n",
        "# Preprocess the text: make it lowercase, remove punctuation, and split into words (array of strings)\n",
        "words_df = (text_file\n",
        "    .select(split(lower(regexp_replace(col(\"value\"), \"[^a-zA-Z\\\\s]\", \"\")), \"\\\\s+\").alias(\"words\"))\n",
        "    .filter(size(col(\"words\")) > 0)  # Filter out empty arrays\n",
        ")\n",
        "\n",
        "# Remove stop words using Spark's StopWordsRemover\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_words_df = remover.transform(words_df)\n",
        "\n",
        "# Explode the filtered words back into individual rows\n",
        "exploded_words_df = filtered_words_df.select(explode(col(\"filtered_words\")).alias(\"word\"))\n",
        "\n",
        "# Filter out any empty strings or spaces\n",
        "cleaned_words_df = exploded_words_df.filter(trim(col(\"word\")) != \"\")\n",
        "\n",
        "# Perform word count on cleaned words\n",
        "word_counts = (cleaned_words_df\n",
        "    .groupBy(\"word\")\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "    .orderBy(\"count\", ascending=False)\n",
        ")\n",
        "\n",
        "# Show the top 15 words\n",
        "word_counts.show(15)\n",
        "\n",
        "\n",
        "# Stop the Spark session\n",
        "# spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lH3LFTuUzwJ",
        "outputId": "99f457a5-9c2b-4bf4-93dc-9744cc8a63fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|     word|count|\n",
            "+---------+-----+\n",
            "|       mr|  806|\n",
            "|elizabeth|  604|\n",
            "|     said|  405|\n",
            "|    darcy|  380|\n",
            "|      mrs|  357|\n",
            "|     much|  338|\n",
            "|     must|  325|\n",
            "|     miss|  315|\n",
            "|   bennet|  307|\n",
            "|      one|  285|\n",
            "|     jane|  271|\n",
            "|  bingley|  261|\n",
            "|     know|  244|\n",
            "|   though|  233|\n",
            "|    never|  230|\n",
            "+---------+-----+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2.1"
      ],
      "metadata": {
        "id": "pmZZd1skhtxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, lower, regexp_replace, col, count, trim, concat_ws, lead\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCoOccurrence\").getOrCreate()\n",
        "\n",
        "# Read the text file\n",
        "file_path = \"PRIDE AND PREJUDICE.rtf\"\n",
        "text_file = spark.read.text(file_path)\n",
        "\n",
        "# Preprocess the text: make it lowercase, remove punctuation, and split into words (array of strings)\n",
        "words_df = (text_file\n",
        "    .select(split(lower(regexp_replace(col(\"value\"), \"[^a-zA-Z\\\\s]\", \"\")), \"\\\\s+\").alias(\"words\"))\n",
        "    .filter(col(\"words\").isNotNull())  # Filter out any null arrays\n",
        ")\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_words_df = remover.transform(words_df)\n",
        "\n",
        "# Explode words into individual rows\n",
        "exploded_words_df = filtered_words_df.select(explode(col(\"filtered_words\")).alias(\"word\"))\n",
        "\n",
        "# Filter out any empty strings\n",
        "cleaned_words_df = exploded_words_df.filter(trim(col(\"word\")) != \"\")\n",
        "\n",
        "# Generate bigrams using window function\n",
        "window_spec = Window.orderBy(lit(1))  # Using lit(1) to create a global window\n",
        "bigrams_df = (cleaned_words_df\n",
        "    .withColumn(\"next_word\", lead(\"word\", 1).over(window_spec))\n",
        "    .filter(col(\"next_word\").isNotNull())  # Remove rows where next_word is null\n",
        "    .select(concat_ws(\" \", col(\"word\"), col(\"next_word\")).alias(\"bigram\"))\n",
        ")\n",
        "\n",
        "# Count bigrams\n",
        "bigram_counts = (bigrams_df\n",
        "    .groupBy(\"bigram\")\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "    .orderBy(\"count\", ascending=False)\n",
        ")\n",
        "\n",
        "# Show the top 10 bigrams\n",
        "bigram_counts.show(10)\n",
        "\n",
        "# Stop the Spark session\n",
        "# spark.stop()\n"
      ],
      "metadata": {
        "id": "JAx4cqnYVOuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b134a580-d656-41f9-8144-f8d88225ad82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|           bigram|count|\n",
            "+-----------------+-----+\n",
            "|         mr darcy|  242|\n",
            "|       mrs bennet|  147|\n",
            "|       mr collins|  140|\n",
            "|   lady catherine|  106|\n",
            "|       mr bingley|   94|\n",
            "|project gutenberg|   87|\n",
            "|        mr bennet|   82|\n",
            "|     miss bingley|   72|\n",
            "|      miss bennet|   62|\n",
            "|       mr wickham|   57|\n",
            "+-----------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing stop words mr, mrs etc"
      ],
      "metadata": {
        "id": "nRQgcaVekSWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, lower, regexp_replace, col, count, trim, concat_ws, lead\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCoOccurrence\").getOrCreate()\n",
        "\n",
        "# Custom stop words list (adding titles like 'mr', 'mrs', 'lady', 'miss' etc.)\n",
        "custom_stopwords = [\"mr\", \"mrs\", \"lady\", \"miss\", \"sir\", \"lord\", \"countess\", \"duke\", \"king\", \"queen\", \"prince\", \"princess\"]\n",
        "\n",
        "# Read the text file\n",
        "file_path = \"PRIDE AND PREJUDICE.rtf\"\n",
        "text_file = spark.read.text(file_path)\n",
        "\n",
        "# Preprocess the text: make it lowercase, remove punctuation, and split into words (array of strings)\n",
        "words_df = (text_file\n",
        "    .select(split(lower(regexp_replace(col(\"value\"), \"[^a-zA-Z\\\\s]\", \"\")), \"\\\\s+\").alias(\"words\"))\n",
        "    .filter(col(\"words\").isNotNull())  # Filter out any null arrays\n",
        ")\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "remover.setStopWords(remover.getStopWords() + custom_stopwords)  # Add custom stop words\n",
        "filtered_words_df = remover.transform(words_df)\n",
        "\n",
        "# Explode words into individual rows\n",
        "exploded_words_df = filtered_words_df.select(explode(col(\"filtered_words\")).alias(\"word\"))\n",
        "\n",
        "# Filter out any empty strings\n",
        "cleaned_words_df = exploded_words_df.filter(trim(col(\"word\")) != \"\")\n",
        "\n",
        "# Generate bigrams using window function\n",
        "window_spec = Window.orderBy(lit(1))  # Using lit(1) to create a global window\n",
        "bigrams_df = (cleaned_words_df\n",
        "    .withColumn(\"next_word\", lead(\"word\", 1).over(window_spec))\n",
        "    .filter(col(\"next_word\").isNotNull())  # Remove rows where next_word is null\n",
        "    .select(concat_ws(\" \", col(\"word\"), col(\"next_word\")).alias(\"bigram\"))\n",
        ")\n",
        "\n",
        "# Count bigrams\n",
        "bigram_counts = (bigrams_df\n",
        "    .groupBy(\"bigram\")\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "    .orderBy(\"count\", ascending=False)\n",
        ")\n",
        "\n",
        "# Show the top 10 bigrams\n",
        "bigram_counts.show(10)\n",
        "\n",
        "# Stop the Spark session\n",
        "# spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAyQNHt2hyhL",
        "outputId": "49bfcd54-56d4-4ab4-b8a5-c237a2479b13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|           bigram|count|\n",
            "+-----------------+-----+\n",
            "|project gutenberg|   87|\n",
            "|   said elizabeth|   46|\n",
            "|     george allen|   37|\n",
            "|        de bourgh|   36|\n",
            "| copyright george|   35|\n",
            "|        young man|   33|\n",
            "|         dare say|   30|\n",
            "|     young ladies|   28|\n",
            "|  heading chapter|   27|\n",
            "|  colonel forster|   27|\n",
            "+-----------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fn3ZQXbmkXFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}